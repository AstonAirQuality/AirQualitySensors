{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting data from csvs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%store -r sensorIds\r\n",
    "%store -r sensorPaths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "processLocation = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import psycopg2\r\n",
    "from datetime import datetime\r\n",
    "from datetime import timezone\r\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def dataSplit(csvpath,dateString):\r\n",
    "\r\n",
    "    data = {} #intialise empty dictionary to store each day of records\r\n",
    "\r\n",
    "    df_temp = pd.read_csv(csvpath,parse_dates=True, index_col=\"timestamp\")\r\n",
    "\r\n",
    "    #convert index from float to int\r\n",
    "    df_temp.index = df_temp.index.astype(int,copy=False)\r\n",
    "\r\n",
    "    # using the dates which are already supplied. This strategy in the line below converts them and rounds down to date using 'd' flag\r\n",
    "    # This strategy (line below) will keep just the date\r\n",
    "    df_temp['day'] = pd.to_datetime(df_temp[dateString], dayfirst=True, errors='coerce').dt.date\r\n",
    "\r\n",
    "    the_unique_dates = df_temp['day'].unique()\r\n",
    "    #print('Unique dates:',the_unique_dates)\r\n",
    "    # this gives the same result as the for loop below\r\n",
    "  \r\n",
    "    #splitting the dataframe into separate days\r\n",
    "    #for each day in unique dates set:\r\n",
    "    for day in the_unique_dates:\r\n",
    "        try:\r\n",
    "            # In my code below I assign the subset of records to a new dataframe called dft\r\n",
    "            # create 'midnight' timestamps\r\n",
    "            timestampKey = int((pd.to_datetime(day, errors='coerce')).timestamp())\r\n",
    "\r\n",
    "            # select the records for this day\r\n",
    "            dft = df_temp[df_temp['day']==day]\r\n",
    "\r\n",
    "            #drop the day column to save space (we don't need this anymore)\r\n",
    "            dft = dft.drop(\"day\", axis=1)\r\n",
    "\r\n",
    "            data[timestampKey] = dft\r\n",
    "\r\n",
    "        except KeyError as e:\r\n",
    "            print(e)\r\n",
    "    \r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#TODO read json from cypress to get the date range for the data. \r\n",
    "#convert date range into a list of date timestamps (dateList)\r\n",
    "#for each date in dateList check if there is a matching timestampkey in measurement dictionary for each sensor \r\n",
    "#flag the missing days of data \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#checks if the spliited dataframes total records match the original dataframe\r\n",
    "def testSize(my_dictionary,path):\r\n",
    "    split_total = 0\r\n",
    "\r\n",
    "    #extracted dataframes:\r\n",
    "    for timestampKey in my_dictionary:\r\n",
    "        df_temp = my_dictionary[timestampKey]\r\n",
    "        split_total += len(df_temp.index.values)\r\n",
    "\r\n",
    "    #original dataframes:\r\n",
    "    df_temp = pd.read_csv(path,parse_dates=True, index_col=\"timestamp\")\r\n",
    "    total = len(df_temp.index.values)\r\n",
    "\r\n",
    "    #check if the sizes match\r\n",
    "    if split_total != total:\r\n",
    "        print(False)\r\n",
    "        print(\"extracted dataframe size: \" + str(split_total) + \"\\n\" + \"original dataframe size: \" + str(total))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "measurement_dictionary = {}\r\n",
    "location_dictionary = {}\r\n",
    "\r\n",
    "# sensorDictionary = {}\r\n",
    "for key in sensorPaths:\r\n",
    "    value = sensorPaths[key]\r\n",
    "    for i in range(len(value)):\r\n",
    "        if i == 0:\r\n",
    "            measurement_dictionary[key] = dataSplit(value[i],\"date\")\r\n",
    "            #test size per sensor\r\n",
    "            testSize(measurement_dictionary[key],value[i])\r\n",
    "        else:\r\n",
    "            location_dictionary[key] = dataSplit(value[i],\"date\")\r\n",
    "            #test size per sensor\r\n",
    "            testSize(location_dictionary[key],value[i])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#disable location procesdsing if no location csv found\r\n",
    "if len(location_dictionary) == 0:\r\n",
    "    processLocation = False \r\n",
    "    print(\"No locations found\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting hourly averages for each day"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def hourlyAverage(dictionary,measure_dict_type):\r\n",
    "    daily_averages_dictionary = {}\r\n",
    "\r\n",
    "    df = pd.DataFrame\r\n",
    "\r\n",
    "        #loop through each day of data \r\n",
    "    for timestampKey in dictionary:\r\n",
    "            \r\n",
    "        df = dictionary[timestampKey]\r\n",
    "        df = df.set_index('date')\r\n",
    "        df.index = pd.to_datetime(df.index, dayfirst=True)\r\n",
    "        df.sort_index()\r\n",
    "\r\n",
    "        #resample min max if using location dictionary or mean if using measurement dictionary\r\n",
    "        if measure_dict_type == True:\r\n",
    "            df = df.resample('60min').mean()\r\n",
    "        else:\r\n",
    "            df = df.resample('60min').agg(['min','max']) \r\n",
    "\r\n",
    "            hoursummary = []\r\n",
    "            #loop through each hour of data\r\n",
    "            try:\r\n",
    "                for row in df.iterrows():\r\n",
    "                    min_x= row[1][0]    # lat\r\n",
    "                    max_x = row[1][1]   # lat\r\n",
    "                    \r\n",
    "                    min_y= row[1][2]    # long\r\n",
    "                    max_y = row[1][3]   # long\r\n",
    "\r\n",
    "                    geometry_string = \"POLYGON(({} {}, {} {}, {} {}, {} {},{} {}))\".format(min_x,min_y,   min_x,max_y,   max_x,max_y,   max_x,min_y,   min_x,min_y)\r\n",
    "\r\n",
    "                    \r\n",
    "                    hoursummary.append((row[0],geometry_string))\r\n",
    "    \r\n",
    "            except Exception as e:\r\n",
    "                print('The dataframe is empty therefore no bounding box will be applied :{}'.format(e))\r\n",
    "\r\n",
    "            df = pd.DataFrame.from_records(hoursummary,columns=['date', 'boundingBox'])\r\n",
    "            df = df.set_index('date')\r\n",
    "    \r\n",
    "    #generate timestamp index for new dataframe\r\n",
    "    df['timestamp'] = df.index.values.astype(np.int64) // 10 ** 9\r\n",
    "    df = df.set_index('timestamp')\r\n",
    "    daily_averages_dictionary[timestampKey] = df   #assign new dataframe to coressponding key\r\n",
    "\r\n",
    "    return daily_averages_dictionary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sensor_dictionary = {}\r\n",
    "measurement_averages_dictionary = {}\r\n",
    "location_averages_dictionary = {}\r\n",
    "\r\n",
    "for key in sensorIds:\r\n",
    "    hourly_dictionary = {}\r\n",
    "\r\n",
    "    measurement_averages_dictionary[key] = hourlyAverage(measurement_dictionary[key],True)\r\n",
    "    \r\n",
    "    if processLocation == True:\r\n",
    "        location_averages_dictionary[key] = hourlyAverage(location_dictionary[key],False)\r\n",
    "        #df = hourlyAverage(sensorIds,location_dictionary,False)\r\n",
    "        \r\n",
    "        for key in measurement_averages_dictionary:\r\n",
    "            for timestampKey in measurement_averages_dictionary[key]:\r\n",
    "                try:\r\n",
    "                    tempdf = measurement_averages_dictionary[key][timestampKey]\r\n",
    "                    tempdf2 = location_averages_dictionary[key][timestampKey]\r\n",
    "                    \r\n",
    "                    #merge both location and measurement into one dataframe\r\n",
    "                    df = pd.concat([tempdf2, tempdf], axis=1)\r\n",
    "\r\n",
    "                    hourly_dictionary[timestampKey] = df\r\n",
    "                except Exception as e:\r\n",
    "                    print('Location data unavailable for this day {} : {}'.format(str(timestampKey),e))\r\n",
    "sensor_dictionary[key] = hourly_dictionary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "if processLocation == True:\r\n",
    "    for key in sensor_dictionary:\r\n",
    "        for timestampKey in sensor_dictionary[key]:\r\n",
    "            tempdf = sensor_dictionary[key][timestampKey]\r\n",
    "        break\r\n",
    "\r\n",
    "    tempdf.head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing data for upload into PostGres"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "sensor_summaries = {}\r\n",
    "sensor_data = {}\r\n",
    "\r\n",
    "for key in sensorIds:\r\n",
    "\r\n",
    "    ldf = mdf =  pd.DataFrame\r\n",
    "    geometry_string = \"\"\r\n",
    "    timestamp_sensor_key = \"\"\r\n",
    "\r\n",
    "    #looking at each day of data \r\n",
    "    for timestampKey in measurement_dictionary[key]:\r\n",
    "   \r\n",
    "        # concatenating numbers into text: \r\n",
    "        timestamp_sensor_key = \"%s_%s\" % (timestampKey, key)\r\n",
    "\r\n",
    "        #only process locations if they exist \r\n",
    "        #if no location exists then use empty dataframe and empty geometry string\r\n",
    "        if processLocation == True:\r\n",
    "            #try extract the bounding box for the current day of data\r\n",
    "            try:\r\n",
    "                #location dataframe\r\n",
    "                ldf = location_dictionary[key][timestampKey]\r\n",
    "\r\n",
    "                #create bounding box polygon\r\n",
    "                min_y= ldf['longitude'].min()\r\n",
    "                max_y = ldf['longitude'].max()\r\n",
    "\r\n",
    "                min_x= ldf['latitude'].min()\r\n",
    "                max_x = ldf['latitude'].max()\r\n",
    "\r\n",
    "                ldf = ldf.drop('date', axis=1)\r\n",
    "                #POLYGON(minx miny, minx Maxy, maxx Maxy, maxx miny, minx miny)\r\n",
    "                geometry_string = \"POLYGON(({} {}, {} {}, {} {}, {} {},{} {}))\".format(min_x,min_y,   min_x,max_y,   max_x,max_y,   max_x,min_y,   min_x,min_y)\r\n",
    "            \r\n",
    "            except Exception as e:\r\n",
    "                print('No location dataframe found at timestampkey: {} for sensor {}'.format(e,key))\r\n",
    "\r\n",
    "                geometry_string = 'NULL'\r\n",
    "                #create an empty dataframe with the necessary columns which can be converted to a Json\r\n",
    "                d = {'timestamp': [timestampKey], 'latitude': [np.NaN], 'longitude': [np.NaN]}\r\n",
    "                ldf = pd.DataFrame(data=d)\r\n",
    "                ldf.set_index('timestamp', inplace=True)\r\n",
    "\r\n",
    "        else:\r\n",
    "            geometry_string = 'NULL'\r\n",
    "            #create an empty dataframe with the necessary columns which can be converted to a Json\r\n",
    "            d = {'timestamp': [timestampKey], 'latitude': [np.NaN], 'longitude': [np.NaN]}\r\n",
    "            ldf = pd.DataFrame(data=d)\r\n",
    "            ldf.set_index('timestamp', inplace=True)\r\n",
    "                \r\n",
    "        # try get measurement dataframe for the current day of data\r\n",
    "        try:\r\n",
    "            #measurement dataframe\r\n",
    "            mdf = measurement_dictionary[key][timestampKey] \r\n",
    "            mdf = mdf.drop('date', axis=1)\r\n",
    "        except Exception as e:\r\n",
    "             print('No measurement dataframe found at timestampkey: {} for sensor {}'.format(e,key))\r\n",
    "\r\n",
    "\r\n",
    "        #summaryArray = [timestamp_start,sensor_id,bouding_box,measurement_count]\r\n",
    "        summaryArray = [timestampKey,int(key),geometry_string,len(mdf.index.values)] #inserting row into temp array\r\n",
    "        sensor_summaries[timestamp_sensor_key] = summaryArray    #assign new dataframe to coressponding key\r\n",
    "\r\n",
    "        #dataArray = [id, mesaurement_json,location_json]\r\n",
    "        dataArray = [mdf.to_json(orient=\"columns\"),ldf.to_json(orient=\"columns\")]\r\n",
    "        sensor_data[timestamp_sensor_key] = dataArray    #assign new dataframe to coressponding key    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exporting to PostgresSQL \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Writing records from a Dictionary of arrays to a SQL database\n",
    "loop over all the keys and execute insert query"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#Connecting to an existing database\r\n",
    "con = psycopg2.connect(\r\n",
    "    host=\"localhost\",\r\n",
    "    database=\"airQuality\",\r\n",
    "    user=\"Riyad\", \r\n",
    "    password=\"123\",\r\n",
    "    # attempt to connect for 3 seconds then raise exception\r\n",
    "    connect_timeout = 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Opening a cursor to execute database operations\r\n",
    "cursor = con.cursor()\r\n",
    "query = \"SELECT * FROM sensor_network.sensors\"\r\n",
    "#change dataframe to csv and save file\r\n",
    "sensorsdf = pd.read_sql_query(query, con, index_col='plume_id')\r\n",
    "sensorsdf = sensorsdf.convert_dtypes() #convert to correct types\r\n",
    "cursor.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "sensorsdf.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_serial_number</th>\n",
       "      <th>id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>zephyr_id</th>\n",
       "      <th>active</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plume_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16701.0</th>\n",
       "      <td>02:00:00:00:40:45</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18683.0</th>\n",
       "      <td>02:00:00:00:48:03</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720.0</th>\n",
       "      <td>02:00:00:00:48:28</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749.0</th>\n",
       "      <td>02:00:00:00:48:45</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18704.0</th>\n",
       "      <td>02:00:00:00:48:18</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sensor_serial_number  id  type_id  zephyr_id  active last_update\n",
       "plume_id                                                                 \n",
       "16701.0     02:00:00:00:40:45   5        1       <NA>   False        None\n",
       "18683.0     02:00:00:00:48:03   6        1       <NA>   False        None\n",
       "18720.0     02:00:00:00:48:28   8        1       <NA>   False        None\n",
       "18749.0     02:00:00:00:48:45   9        1       <NA>   False        None\n",
       "18704.0     02:00:00:00:48:18  10        1       <NA>   False        None"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "for key in sensor_summaries:\r\n",
    "\r\n",
    "    #split key to get only sensorid\r\n",
    "    s = key.split('_')[1]\r\n",
    "    #get the new key from sensors table\r\n",
    "    sensor_id = sensorsdf.loc[int(s)]['id']\r\n",
    "\r\n",
    "    summary_df = sensor_summaries[key]\r\n",
    "    data_df = sensor_data[key]\r\n",
    "\r\n",
    "    try:\r\n",
    "        #Opening a cursor to execute database operations\r\n",
    "        cursor = con.cursor()\r\n",
    "\r\n",
    "        #inserting sensor data and return the id of new record\r\n",
    "        cursor.execute(\"INSERT INTO sensor_data.archive_measurements (measurements,locations) VALUES(%s, %s) \\n RETURNING id\", (str(data_df[0]),str(data_df[1])) )\r\n",
    "        con.commit() \r\n",
    "        \r\n",
    "        #set id of new record into local variable\r\n",
    "        sensor_data_id = cursor.fetchone()[0]\r\n",
    "\r\n",
    "        #inserting sensor summary\r\n",
    "        if summary_df[2] == 'NULL':\r\n",
    "            cursor.execute(\"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES(%s, %s, NULL, %s, %s)\", (int(summary_df[0]), int(sensor_id),int(sensor_data_id),int(summary_df[3])))\r\n",
    "        else:\r\n",
    "            cursor.execute(\"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES(%s, %s, %s, %s, %s)\", (int(summary_df[0]), int(sensor_id),str(summary_df[2]),int(sensor_data_id),int(summary_df[3])))\r\n",
    "        \r\n",
    "        con.commit() \r\n",
    "\r\n",
    "        # update sensor table \r\n",
    "        current_date = datetime.fromtimestamp(int(key.split('_')[0]))\r\n",
    "        cursor.execute(\"UPDATE sensor_network.sensors SET last_update = %s WHERE id = %s\", (current_date,int(sensor_id)))\r\n",
    "        con.commit() \r\n",
    "\r\n",
    "        cursor.close()\r\n",
    "    #if table name does not exist exit loop \r\n",
    "    except(psycopg2.errors.UndefinedTable) as error:\r\n",
    "        print('ERROR: ' + error)\r\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#closing the connection\r\n",
    "con.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58e6f625ba2c43b7fc3c95c61d4ef7749e639023949ab2a4f0956b7241794ddb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}