{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "58e6f625ba2c43b7fc3c95c61d4ef7749e639023949ab2a4f0956b7241794ddb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PostGreSQL in Jupyter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\r\n",
    "import psycopg2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "%store -r sensorIds\r\n",
    "print(sensorIds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['18699', '18720', '18749']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#reads in hashmap containing paths to csv files. The keys of these paths are represented as the sensor serial numbers\r\n",
    "%store -r hashmap\r\n",
    "print(hashmap)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'18699': '..\\\\data\\\\flow\\\\task_177_1627319652\\\\sensor_18699\\\\sensor_measures_20210717_20210721_1.csv', '18720': '..\\\\data\\\\flow\\\\task_177_1627319652\\\\sensor_18720\\\\sensor_measures_20210711_20210721_1.csv', '18749': '..\\\\data\\\\flow\\\\task_177_1627319652\\\\sensor_18749\\\\sensor_measures_20210716_20210721_1.csv'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#bringing the csv to pandas dataframes\r\n",
    "dfList = []\r\n",
    "for snum in sensorIds:\r\n",
    "    dfList.append(pd.read_csv(hashmap[snum],parse_dates=True, index_col=\"date\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#Connecting to an existing database\r\n",
    "con = psycopg2.connect(\r\n",
    "    host=\"localhost\",\r\n",
    "    database=\"sdb_airQuality\",\r\n",
    "    user=\"Riyad\", \r\n",
    "    password=\"123\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "#Opening a cursor to execute database operations\r\n",
    "cur = con.cursor()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Measurement data\n",
    "## Reading the database \n",
    "\n",
    "We have to check if there is already an existing measurement table before we add the new data. <br>\n",
    "if there is already a table with the same name, then we need to subset the data from the dataframe removing all the data up to the last entry from the database. We can then upload the new dataframe as a seperate table and execute an SQL union command to join the tables together. <br>\n",
    "else, the table does not exist already and therefore we can just upload the entire dataframe as a new table."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from psycopg2 import sql\r\n",
    "\r\n",
    "bollarray = []\r\n",
    "tableExist = True\r\n",
    "\r\n",
    "for snum in serialnums: \r\n",
    "    try: \r\n",
    "        cur.execute(sql.SQL(\"SELECT * FROM {}\").format(sql.Identifier(Username)))\r\n",
    "    except psycopg2.Error as e:\r\n",
    "        if e.pgcode == \"25P02\":\r\n",
    "            print(\"No table exists yet, importing a new table\")\r\n",
    "            tableExist = False\r\n",
    "        else:\r\n",
    "            print(\"An unexpected error has occured, Error code: \" + e.pgcode)\r\n",
    "\r\n",
    "#cur.execute(sql.SQL(\"SELECT * FROM {} WHERE id = %s\").format(sql.Identifier(Username)),[cur.rowcount-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "cur.rowcount"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3942"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the table already exists we simply add _clone to the new table we will add"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#if the table exists then we should get the rows after the last entry which exists in the database.\r\n",
    "if tableExist == True:\r\n",
    "    print(\"This table already exists, Creating clone table . . .\")\r\n",
    "    df = df[df.id > cur.rowcount - 1]\r\n",
    "    #print(smoothdf.head(1))\r\n",
    "    Username += \"_clone\"\r\n",
    "    \r\n",
    "else:\r\n",
    "    print(\"Continue from next code block\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Continue from next code block\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(\"dataframe is ready for upload\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataframe is ready for upload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# we would need to send the data as a new table then do uninion operation to join the data\r\n",
    "# #https://mode.com/sql-tutorial/sql-union/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "cur.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Writing records from a DataFrame to a SQL database\n",
    "Using create_engine() from sqlaclhemy we can generate and execute an SQL query to store the entire dataframe into a table "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sqlalchemy import create_engine"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#engine = create_engine('postgresql://username:password@localhost:5432/mydatabase')\r\n",
    "engine = create_engine('postgresql+psycopg2://Riyad:123@localhost/sdb_airQuality')\r\n",
    "df.to_sql(Username, engine)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "con.commit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "con.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}