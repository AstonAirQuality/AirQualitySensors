{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This CSV version is not supported by zephyr API yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import wget # pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.earthsense.co.uk/dataForViewBySlots/AstonUniversity/Xo08R83d43e0Kk6/814/20210917000/202109200000/B/def/json/api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io,csv\n",
    "from typing import List,Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, stream=True)\n",
    "if res.ok:\n",
    "    file_ = io.BytesIO(res.content)\n",
    "else:\n",
    "     raise IOError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = io.StringIO(file_.read().decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.StringIO at 0x1496d141dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZepyhrSensor:\n",
    "    \"\"\"Per sensor object designed to wrap the csv files returned by the Plume API.\n",
    "\n",
    "    Example Usage:\n",
    "        ps = PlumeSensor.from_csv(\"16397\", open(\"sensor_measures_20211004_20211008_1.csv\"))\n",
    "        print(ps.DataFrame)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, id_, header: List, rows: List):\n",
    "        self.id = id_\n",
    "        self.header = header\n",
    "        self.rows = rows\n",
    "\n",
    "    def add_row(self, row: Iterable):\n",
    "        \"\"\"Normalise and append row to internal list.\n",
    "\n",
    "        Coverts all digits to int objects, all elements are initially converted to strings before\n",
    "        digit check to avoid type errors.\n",
    "\n",
    "        :param row: row to add to plume sensor\n",
    "        \"\"\"\n",
    "        self.rows.append([int(i) if str(i).isdigit() else i for i in row])\n",
    "    \n",
    "    @property\n",
    "    def dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Writes headers and rows into a DataFrame.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(self.rows, columns=self.header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_id = '814'\n",
    "reader = csv.reader(buffer, dialect=csv.unix_dialect)\n",
    "header = next(reader)\n",
    "sensor = ZepyhrSensor(sensor_id, header, [])\n",
    "for row in reader:\n",
    "    sensor.add_row(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>{\"slotB\": {\"NO\": {\"header\": {\"key\": \"NO\"</th>\n",
       "      <th>\"group\": \"NO\"</th>\n",
       "      <th>\"label\": \"NO\"</th>\n",
       "      <th>\"units\": \"ug/m3\"</th>\n",
       "      <th>\"CSVOrder\": 6</th>\n",
       "      <th>\"HTMLLabel\": \"NO\"}</th>\n",
       "      <th>\"data\": [0</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>\"&lt;/p&gt;\"]</th>\n",
       "      <th>\"errors\": []</th>\n",
       "      <th>\"timings\": {\"generateCalibratedData\": 3.926924</th>\n",
       "      <th>\"overall\": 4.848803</th>\n",
       "      <th>\"generateCalibratedData_getRawData_fromDB\": 2.564961</th>\n",
       "      <th>\"generateCalibratedData_runCDGs\": 0.531226</th>\n",
       "      <th>\"generateCalibratedData_getRawData_process\": 0.752605</th>\n",
       "      <th>\"processAndPackageCalibratedData\": 0.468759}</th>\n",
       "      <th>\"datahash\": \"9f92745d\"</th>\n",
       "      <th>\"indent\": 0}}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 355197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [{\"slotB\": {\"NO\": {\"header\": {\"key\": \"NO\",  \"group\": \"NO\",  \"label\": \"NO\",  \"units\": \"ug/m3\",  \"CSVOrder\": 6,  \"HTMLLabel\": \"NO\"},  \"data\": [0,  0,  2,  0,  13,  2,  0,  0,  1,  0,  21,  0,  0,  0,  0,  1,  0,  3,  0,  0,  0,  2,  0,  0,  0,  8,  0,  0,  20,  0,  0,  12,  0,  0,  0,  12,  2,  0,  5,  0,  0,  0,  18,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  12,  0,  0,  1,  0,  0,  5,  0,  14,  9,  0,  0,  0,  0,  0,  12,  3,  0,  0,  3,  6,  0,  0,  0,  16,  4,  0,  0,  0, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 355197 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor.dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zephyr_username = \"AstonUniversity\"\n",
    "zephyr_password = \"Xo08R83d43e0Kk6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.earthsense.co.uk/zephyrsForUser/%s/%s\" % (zephyr_username, zephyr_password)\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jprint(obj):\n",
    "    # create a formatted string of the Python JSON object\n",
    "    zepyhr_id = json.dumps(obj, sort_keys=True, indent=4)\n",
    "    return zepyhr_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_times = response.json()['usersZephyrs']['13883']['zNumber']\n",
    "jprint(pass_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonObject = response.json()['usersZephyrs']\n",
    "zephyr_id_list = []\n",
    "\n",
    "for key in jsonObject:\n",
    "    pass_times = response.json()['usersZephyrs'][key]['zNumber']\n",
    "    zephyr_id_list.append(jprint(pass_times))\n",
    "    \n",
    "print(zephyr_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_datetime = \"202108180000\"\n",
    "end_datetime = \"202108200000\"\n",
    "slots = \"B\"\n",
    "format_output = \"csv\"\n",
    "target = \"api\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for zephyr_id in zephyr_id_list:\n",
    "    url = \"https://data.earthsense.co.uk/dataForViewBySlots/%s/%s/%s/%s/%s/%s/def/%s/%s\" % (zephyr_username, zephyr_password, zephyr_id,start_datetime,end_datetime,slots,format_output,target)\n",
    "\n",
    "    #We can use the wget library to download the zip file.\n",
    "    #We can then extract the file into a local \"data\" folder \n",
    "    #The final step is to identify the paths of the csv files we need.\n",
    "    try: \n",
    "        target = wget.download(url, \"..\\data\\zephyrs\")\n",
    "    except BaseException as error:\n",
    "        print('An exception occurred: {}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zephyr_csv = []\n",
    "\n",
    "for zephyr_id in zephyr_id_list:\n",
    "    url = \"https://data.earthsense.co.uk/dataForViewBySlots/%s/%s/%s/%s/%s/%s/def/%s/%s\" % (zephyr_username, zephyr_password, zephyr_id,start_datetime,end_datetime,slots,format_output,target)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        zephyr_jsons.append(response.json()['slotB'])\n",
    "    else:\n",
    "        print(\"HTTP request error code: \" + response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json to dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and preparing the dataframe from the json objects\n",
    "dfList = []\n",
    "for jsonObject in zephyr_jsons:\n",
    "    df_temp = pd.DataFrame.from_records(jsonObject)\n",
    "    df_temp.drop('header', axis =0, inplace=True)\n",
    "    df_temp.drop('data_hash', axis =0, inplace=True)\n",
    "    df_temp.drop('UTS', axis=1, inplace=True)\n",
    "\n",
    "    #explode function transform each element of a list to a row.\n",
    "    #we can apply it to all the columns assuming they have the same number of elements in each list \n",
    "    df_temp = df_temp.apply(pd.Series.explode)\n",
    "    \n",
    "    dfList.append(df_temp)\n",
    "\n",
    "measurement_dictionary = {k:v for k,v in zip(zephyr_id_list,dfList)}\n",
    "dfList = [] # clear dataframe list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataframe into days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(df_temp,dateString):\n",
    "    \n",
    "    data = {} #intialise empty dictionary to store each day of locations\n",
    "    \n",
    "    df_temp = measurement_dictionary[key]\n",
    "\n",
    "    # using the dates which are already supplied. This strategy in the line below converts them and rounds down to date using 'd' flag\n",
    "    # This strategy (line below) will keep just the date\n",
    "    df_temp['day'] = pd.to_datetime(df_temp[dateString], dayfirst=True, errors='coerce').dt.date\n",
    "\n",
    "    the_unique_dates = df_temp['day'].unique()\n",
    "    #print('Unique dates:',the_unique_dates)\n",
    "    # this gives the same result as the for loop below\n",
    "    \n",
    "    #splitting the dataframe into separate days\n",
    "    #for each day in unique dates set:\n",
    "    for day in the_unique_dates:\n",
    "        try:\n",
    "            # In my code below I assign the subset of records to a new dataframe called dft\n",
    "            # create 'midnight' timestamps\n",
    "            timestampKey = int((pd.to_datetime(day, errors='coerce')).timestamp())\n",
    "\n",
    "            # select the records for this day\n",
    "            dft = df_temp[df_temp['day']==day]\n",
    "\n",
    "            # #drop the date column to save space (we don't need this anymore)\n",
    "            #dft = dft.drop(dateString, axis=1)\n",
    "            dft = dft.drop(\"day\", axis=1)\n",
    "            dft = dft.dropna() #drop null values\n",
    "            \n",
    "            data[timestampKey] = dft\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurement_dictionary:\n",
    "    measurement_dictionary[key] = dataSplit(measurement_dictionary[key],'dateTime')\n",
    "\n",
    "location_dictionary = measurement_dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating each list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTimestamps(df):\n",
    "    #set the index to Timestamp and automatically drops the datetime index\n",
    "    df['Timestamp'] = df.index.values.astype(np.int64) // 10 ** 9\n",
    "    df.set_index('Timestamp',inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframeAverages(intervalString,df):\n",
    "    #convert all column types to int\n",
    "    df = df.astype(int)\n",
    "    #resample by minute and get mean\n",
    "    df= df.resample(intervalString).mean() \n",
    "\n",
    "    df.isnull().sum() # identify any null values\n",
    "    df.dropna(inplace=True) #drop null values\n",
    "    #df = df.astype(int)  #convert avergaes back to int\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurement_dictionary:\n",
    "    for timestamp in measurement_dictionary[key]:\n",
    "        df_temp = measurement_dictionary[key][timestamp]\n",
    "        df_temp.set_index('dateTime',inplace=True)\n",
    "        df_temp.index = pd.to_datetime(df_temp.index,dayfirst=True)\n",
    "\n",
    "        #extracting the location data\n",
    "        ldf = df_temp[['latitude','longitude']]\n",
    "        df_temp.drop(['latitude','longitude'],axis=1, inplace=True) #drop the lcoation columns \n",
    "\n",
    "        #calculate minute averages\n",
    "        #df_temp = dataframeAverages('1min',df_temp)\n",
    "        #df_temp = calculateTimestamps(df_temp)\n",
    "\n",
    "        #reassign new location dataframe to dictionary\n",
    "        location_dictionary[key][timestamp] = ldf\n",
    "\n",
    "        #reassign new measurement dataframe to dictionary\n",
    "        measurement_dictionary[key][timestamp] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurement_dictionary:\n",
    "    for timestamp in measurement_dictionary[key]:\n",
    "        df_temp = measurement_dictionary[key][timestamp]\n",
    "    #break\n",
    "\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframeAverages('60min',df_temp)\n",
    "\n",
    "myTimeFormat = mdates.DateFormatter('%D:%H:%M')\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig = plt.figure(figsize = (24,20))\n",
    "axs = fig.add_suplot(111)\n",
    "\n",
    "axs.set_title('PM partiuclate hourly average')\n",
    "axs.plot(df.index, df[\"particulatePM1\"], color = (0.4,0.4,0.4), linewidth = 4, alpha = .9, label = 'pm 1')\n",
    "axs.plot(df.index, df[\"particulatePM25\"], color = (0.6,0.6,0.9), linewidth = 4, alpha = .9, label = 'pm 2.5')\n",
    "#axs.plot(df.index, df[\"particulatePM10\"], color = (0.9,0.6,0.6), linewidth = 4, alpha = .9, label = 'pm 10')\n",
    "axs.set_xlabel(\"Time\")\n",
    "axs.set_ylabel(\"particualte matter (ug/m3)\")\n",
    "axs.xaxis.set_major_formatter(myTimeFormat)\n",
    "axs.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSensorMovement(ldf):\n",
    "    precision = 0.00025 #this precision constant is derived from the GPS min and max readings which have 0.0003+/- variance for a stationary sensor \n",
    "\n",
    "    full_location_storage = False\n",
    "\n",
    "    #check if the sensor has moved greater than a speicifed precision. \n",
    "    if ((ldf['latitude'].min() + precision) > ldf['latitude'].max()) or ((ldf['latitude'].max() - precision) < ldf['latitude'].min()):\n",
    "        full_location_storage = True\n",
    "    elif ((ldf['longitude'].min() + precision) > ldf['longitude'].max()) or ((ldf['longitude'].max() - precision) < ldf['longitude'].min()):\n",
    "        full_location_storage = True\n",
    "    else:\n",
    "        full_location_storage = False\n",
    "\n",
    "    return full_location_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBoundingBox(ldf):\n",
    "    min_y= ldf['longitude'].min()\n",
    "    max_y = ldf['longitude'].max()\n",
    "\n",
    "    min_x= ldf['latitude'].min()\n",
    "    max_x = ldf['latitude'].max()\n",
    "    geometry_string = \"POLYGON(({} {}, {} {}, {} {}, {} {},{} {}))\".format(min_x,min_y,   min_x,max_y,   max_x,max_y,   max_x,min_y,   min_x,min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "hoursummary = []\n",
    "\n",
    "ldf = ldf.resample('60min').agg(['min','max']) #resample only works with datetimes\n",
    "ldf['Timestamp'] = ldf.index.values.astype(np.int64) // 10 ** 9\n",
    "ldf.set_index('Timestamp',inplace=True)\n",
    "\n",
    "    #extracting a bounding box from hourly readings\n",
    "try:\n",
    "    for row in ldf.iterrows():\n",
    "        geometry_string = \"\"\n",
    "\n",
    "        tempArray = [] \n",
    "        min_x= row[1][0]    # lat\n",
    "        max_x = row[1][1]   # lat\n",
    "                \n",
    "        min_y= row[1][2]    # long\n",
    "        max_y = row[1][3]   # long\n",
    "\n",
    "        geometry_string = \"POLYGON(({} {}, {} {}, {} {}, {} {},{} {}))\".format(min_x,min_y,   min_x,max_y,   max_x,max_y,   max_x,min_y,   min_x,min_y)\n",
    "\n",
    "        tempArray = [geometry_string,len(ldf.index.values)] #inserting row into temp array\n",
    "    hoursummary.append(tempArray)\n",
    "\n",
    "except Exception as e:\n",
    "    print('The dataframe is empty therefore no bounding box will be applied :{0}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hoursummary[0])\n",
    "#ldf.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58e6f625ba2c43b7fc3c95c61d4ef7749e639023949ab2a4f0956b7241794ddb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
