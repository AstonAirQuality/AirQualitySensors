{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'email': 'AstonUniversity', 'password': 'Xo08R83d43e0Kk6', 'dateStart': '20210917000', 'dateEnd': '202109200000', 'sensors': ['814', '821'], 'slots': 'B', 'format_output': 'json', 'target': 'api'}\n"
     ]
    }
   ],
   "source": [
    "with open('..\\\\..\\\\CypressAutomation\\\\cypress\\\\fixtures\\\\Zephyr.json') as f:\n",
    "    requestData = json.load(f)\n",
    "requestData['slots'] = \"B\"\n",
    "requestData['format_output'] = \"json\"\n",
    "requestData['target'] = \"api\"\n",
    "print(requestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zephyr_jsons = []\n",
    "\n",
    "for zephyr_id in requestData['sensors']:\n",
    "    url = \"https://data.earthsense.co.uk/dataForViewBySlots/%s/%s/%s/%s/%s/%s/def/%s/%s\" % (requestData['email'], requestData['password'], zephyr_id,requestData['dateStart'],requestData['dateEnd'],requestData['slots'],requestData['format_output'],requestData['target'])\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        zephyr_jsons.append(response.json()['slotB'])\n",
    "    else:\n",
    "        print(\"HTTP request error code: \" + response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json to dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting and preparing the dataframe from the json objects\n",
    "dfList = []\n",
    "for jsonObject in zephyr_jsons:\n",
    "    df_temp = pd.DataFrame.from_records(jsonObject)\n",
    "    df_temp.drop('header', axis =0, inplace=True)\n",
    "    df_temp.drop('data_hash', axis =0, inplace=True)\n",
    "    df_temp.drop('UTS', axis=1, inplace=True)\n",
    "\n",
    "    #explode function transform each element of a list to a row.\n",
    "    #we can apply it to all the columns assuming they have the same number of elements in each list \n",
    "    df_temp = df_temp.apply(pd.Series.explode)\n",
    "    \n",
    "    dfList.append(df_temp)\n",
    "\n",
    "measurement_dictionary = {k:v for k,v in zip(requestData['sensors'],dfList)}\n",
    "dfList = [] # clear dataframe list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataframe into days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(df_temp,dateString):\n",
    "    \n",
    "    data = {} #intialise empty dictionary to store each day of locations\n",
    "    \n",
    "    df_temp = measurement_dictionary[key]\n",
    "\n",
    "    # using the dates which are already supplied. This strategy in the line below converts them and rounds down to date using 'd' flag\n",
    "    # This strategy (line below) will keep just the date\n",
    "    df_temp['day'] = pd.to_datetime(df_temp[dateString], dayfirst=True, errors='coerce').dt.date\n",
    "\n",
    "    the_unique_dates = df_temp['day'].unique()\n",
    "    # this gives the same result as the for loop below\n",
    "    \n",
    "    #splitting the dataframe into separate days\n",
    "    #for each day in unique dates set:\n",
    "    for day in the_unique_dates:\n",
    "        try:\n",
    "            # In my code below I assign the subset of records to a new dataframe called dft\n",
    "            # create 'midnight' timestamps\n",
    "            timestampKey = int((pd.to_datetime(day, errors='coerce')).timestamp())\n",
    "\n",
    "            # select the records for this day\n",
    "            dft = df_temp[df_temp['day']==day]\n",
    "\n",
    "            # #drop the date column to save space (we don't need this anymore)\n",
    "            #dft = dft.drop(dateString, axis=1)\n",
    "            dft = dft.drop(\"day\", axis=1)\n",
    "            dft = dft.dropna() #drop null values\n",
    "            \n",
    "            data[timestampKey] = dft\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurement_dictionary:\n",
    "    measurement_dictionary[key] = dataSplit(measurement_dictionary[key],'dateTime')\n",
    "\n",
    "#location_dictionary = measurement_dictionary \n",
    "location_dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flagging missed days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateStart = datetime.strptime(str(requestData['dateStart']) ,'%Y%m%d%H%M').date()\n",
    "dateEnd = datetime.strptime(str(requestData['dateEnd']) ,'%Y%m%d%H%M').date()\n",
    "\n",
    "dateList = pd.date_range(dateStart,dateEnd,freq='d')\n",
    "\n",
    "timeStampRange = []\n",
    "for day in dateList:\n",
    "    timeStampRange.append(int(day.replace(tzinfo=timezone.utc).timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing days for sensor 814 :\n",
      "2021-09-20\n",
      "Missing days for sensor 821 :\n",
      "2021-09-20\n"
     ]
    }
   ],
   "source": [
    "for key in requestData['sensors']:\n",
    "    timestampKeys = list(measurement_dictionary[key])\n",
    "\n",
    "    #if there are no missing days skip to the next sensor\n",
    "    if timeStampRange == timestampKeys:\n",
    "        continue\n",
    "\n",
    "    #flag the missing days\n",
    "    else:\n",
    "        missingDays = list(set(timeStampRange) - set(timestampKeys))\n",
    "        print(\"Missing days for sensor %s :\" % str(key) )\n",
    "        for day in missingDays:\n",
    "            print(datetime.utcfromtimestamp(day).date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1631836800, 1631923200, 1632009600])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_dictionary['814'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating each list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTimestamps(df):\n",
    "    #set the index to Timestamp and automatically drops the datetime index\n",
    "    df['Timestamp'] = df.index.values.astype(np.int64) // 10 ** 9\n",
    "    df.set_index('Timestamp',inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframeAverages(intervalString,df):\n",
    "    #convert all column types to int\n",
    "    df = df.astype(int)\n",
    "    #resample by minute and get mean\n",
    "    df= df.resample(intervalString).mean() \n",
    "\n",
    "    df.isnull().sum() # identify any null values\n",
    "    df.dropna(inplace=True) #drop null values\n",
    "    #df = df.astype(int)  #convert avergaes back to int\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurement_dictionary:\n",
    "    temp_locations = {}\n",
    "    temp_measurements = {}\n",
    "    for timestamp in measurement_dictionary[key]:\n",
    "        df_temp = measurement_dictionary[key][timestamp]\n",
    "        df_temp.set_index('dateTime',inplace=True)\n",
    "        df_temp.index = pd.to_datetime(df_temp.index,dayfirst=True)\n",
    "\n",
    "        #extracting the location data\n",
    "        ldf = df_temp[['latitude','longitude']]\n",
    "        df_temp.drop(['latitude','longitude'],axis=1, inplace=True) #drop the location columns \n",
    "\n",
    "        #calculate minute averages\n",
    "        df_temp = dataframeAverages('1min',df_temp)\n",
    "        df_temp = calculateTimestamps(df_temp)\n",
    "\n",
    "        #reassign new location dataframe to dictionary\n",
    "        temp_locations[timestamp] = ldf\n",
    "\n",
    "        #reassign new measurement dataframe to dictionary\n",
    "        temp_measurements[timestamp] = df_temp\n",
    "\n",
    "    location_dictionary[key] = temp_locations\n",
    "    measurement_dictionary[key] = temp_measurements\n",
    "\n",
    "    temp_locations = {} # clear dictionary \n",
    "    temp_measurements = {} # clear dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSensorMovement(ldf):\n",
    "    precision = 0.00025 #this precision constant is derived from the GPS min and max readings which have 0.0003+/- variance for a stationary sensor \n",
    "\n",
    "    full_location_storage = False\n",
    "\n",
    "    #check if the sensor has moved greater than a speicfied precision. \n",
    "    if ((ldf['latitude'].min() + precision) > ldf['latitude'].max()) or ((ldf['latitude'].max() - precision) < ldf['latitude'].min()):\n",
    "        full_location_storage = True\n",
    "    elif ((ldf['longitude'].min() + precision) > ldf['longitude'].max()) or ((ldf['longitude'].max() - precision) < ldf['longitude'].min()):\n",
    "        full_location_storage = True\n",
    "    else:\n",
    "        full_location_storage = False\n",
    "\n",
    "    return full_location_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBoundingBox(ldf):\n",
    "    for row in ldf.iterrows():\n",
    "        geometry_string = \"\"\n",
    "\n",
    "        tempArray = [] \n",
    "        min_x= row[1][2]    # lat\n",
    "        max_x = row[1][3]   # lat\n",
    "                \n",
    "        min_y= row[1][0]    # long\n",
    "        max_y = row[1][1]   # long\n",
    "\n",
    "    return \"POLYGON((%f %f, %f %f, %f %f, %f %f,%f %f))\" % (min_x,min_y,   min_x,max_y,   max_x,max_y,   max_x,min_y,   min_x,min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_summaries = {}\n",
    "sensor_data = {}\n",
    "\n",
    "for key in requestData['sensors']:\n",
    "    \n",
    "    df = mdf =  pd.DataFrame\n",
    "    geometry_string = \"\"\n",
    "    timestamp_sensor_key = \"\"\n",
    "\n",
    "    #we can look into either dictioanry as they both share the same time range.\n",
    "    for timestampKey in location_dictionary[key]:\n",
    "   \n",
    "        # concatenating numbers into text: \n",
    "        timestamp_sensor_key = \"%s_%s\" % (timestampKey, key)\n",
    "\n",
    "        #try get location dataframe\n",
    "        try:\n",
    "            ldf = location_dictionary[key][timestampKey]\n",
    "            \n",
    "            #check if there is any movment in this current day of data and generate a bounding box if there is \n",
    "            if calculateSensorMovement(ldf) == True:\n",
    "                print(\"Storing all measurements for this moving sensor\")\n",
    "                geometry_string = calculateBoundingBox(ldf)\n",
    "            \n",
    "            #if no movement is recorded them we can just take the min and max measuremnt for the day and drop the other values\n",
    "            else:               \n",
    "                temp_ldf = ldf.resample('D').agg(['min','max']) #resample on day interval\n",
    "                geometry_string = calculateBoundingBox(temp_ldf)\n",
    "               \n",
    "                #convert all column types to float to get an averaged location\n",
    "                ldf = ldf.astype(float)\n",
    "                ldf = ldf.resample('D').mean()\n",
    "                ldf = calculateTimestamps(ldf)\n",
    "               \n",
    "        except Exception as e:\n",
    "            print('The dataframe is empty therefore no bounding box will be applied :{}'.format(e))\n",
    "        \n",
    "        # try get measurement dataframe\n",
    "        try:\n",
    "            mdf = measurement_dictionary[key][timestampKey] \n",
    "        except Exception as e:\n",
    "            print('The measurement dataframe is empty. check csv files :{}'.format(e))\n",
    "\n",
    "\n",
    "        #summaryArray = [timestamp_start,sensor_id,bouding_box,measurement_count]\n",
    "        summaryArray = [timestampKey,int(key),geometry_string,len(mdf.index.values)] #inserting row into temp array\n",
    "        sensor_summaries[timestamp_sensor_key] = summaryArray    #assign new dataframe to coressponding key\n",
    "\n",
    "        #dataArray = [id, mesaurement_json,location_json]\n",
    "        dataArray = [mdf.to_json(orient=\"columns\"),ldf.to_json(orient=\"columns\")] \n",
    "        sensor_data[timestamp_sensor_key] = dataArray    #assign new dataframe to coressponding key    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to PostgresSQL \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing records from a Dictionary of arrays to a SQL database\n",
    "loop over all the keys and execute insert query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO check if sensor is stationary. if it is then don't upload the location table and use the last known stationary geometry string (from our database) in each day of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to an existing database\n",
    "con = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"airQuality\",\n",
    "    user=\"Riyad\", \n",
    "    password=\"123\",\n",
    "    # attempt to connect for 3 seconds then raise exception\n",
    "    connect_timeout = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor = con.cursor()\n",
    "\n",
    "# query = \"UPDATE sensor_network.sensor_summaries SET b_box = 'POLYGON ((52.0 10.0, 51.3 3.61, 51.3 3.0, 52.0 10.0))' WHERE timestamp_start = 1630454400 AND sensor_id = 31 \\n RETURNING sensor_data_id;\"\n",
    "# #query = \"SELECT sensor_data_id FROM sensor_network.sensor_summaries WHERE timestamp_start = 1630454400 AND sensor_id = 31\"\n",
    "# cursor.execute(query)\n",
    "# con.commit()\n",
    "\n",
    "# result = cursor.fetchone()\n",
    "\n",
    "# print('Returns', result)\n",
    "\n",
    "# if result == None:\n",
    "#     #INSERT\n",
    "#     print('INSERT')\n",
    "# else:\n",
    "#     #UPDATE\n",
    "#     print(result)\n",
    "\n",
    "# cursor.close()\n",
    "# con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening a cursor to execute database operations\n",
    "cursor = con.cursor()\n",
    "query = \"SELECT * FROM sensor_network.sensors\"\n",
    "#change dataframe to csv and save file\n",
    "sensorsdf = pd.read_sql_query(query, con, index_col='lookup_id')\n",
    "sensorsdf = sensorsdf.convert_dtypes() #convert to correct types\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_serial_number</th>\n",
       "      <th>id</th>\n",
       "      <th>type_id</th>\n",
       "      <th>active</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lookup_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16701</th>\n",
       "      <td>02:00:00:00:40:45</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18683</th>\n",
       "      <td>02:00:00:00:48:03</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>02:00:00:00:48:28</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18749</th>\n",
       "      <td>02:00:00:00:48:45</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18704</th>\n",
       "      <td>02:00:00:00:48:18</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sensor_serial_number  id  type_id  active last_update\n",
       "lookup_id                                                      \n",
       "16701        02:00:00:00:40:45   5        1   False        None\n",
       "18683        02:00:00:00:48:03   6        1   False        None\n",
       "18720        02:00:00:00:48:28   8        1   False        None\n",
       "18749        02:00:00:00:48:45   9        1   False        None\n",
       "18704        02:00:00:00:48:18  10        1   False        None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensorsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Insertdata(mdf,ldf,sensor_id):\n",
    "#     try:\n",
    "#         #Opening a cursor to execute database operations\n",
    "#         cursor = con.cursor()\n",
    "\n",
    "#         #inserting sensor data and return the id of new record\n",
    "#         cursor.execute(\"INSERT INTO sensor_data.archive_measurements (measurements,locations) VALUES(%s, %s) \\n RETURNING id\", (str(ldf[0]),str(ldf[1])) )\n",
    "#         con.commit() \n",
    "        \n",
    "#         #set id of new record into local variable\n",
    "#         sensor_data_id = cursor.fetchone()[0]\n",
    "\n",
    "#         #inserting a new sensor summary\n",
    "#         cursor.execute(\"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES(%s, %s, %s, %s, %s)\", (int(mdf[0]), int(sensor_id),str(mdf[2]),int(sensor_data_id),int(mdf[3])))\n",
    "#         con.commit() \n",
    "\n",
    "#         #inserting sensor summary\n",
    "#         current_date = datetime.fromtimestamp(int(key.split('_')[0]))\n",
    "#         cursor.execute(\"UPDATE sensor_network.sensors SET last_update = %s WHERE id = %s\", (current_date,int(sensor_id)))\n",
    "#         con.commit() \n",
    "\n",
    "#         cursor.close()\n",
    "#     #if table name does not exist exit loop \n",
    "#     except(psycopg2.errors.UndefinedTable) as error:\n",
    "#         print('ERROR: ' + error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Insertdata(mdf,ldf,sensor_id,update):\n",
    "    try:\n",
    "        #Opening a cursor to execute database operations\n",
    "        cursor = con.cursor()\n",
    "\n",
    "        #inserting sensor data and return the id of new record\n",
    "        cursor.execute(\"INSERT INTO sensor_data.archive_measurements (measurements,locations) VALUES(%s, %s) \\n RETURNING id\", (str(ldf[0]),str(ldf[1])) )\n",
    "        con.commit() \n",
    "        \n",
    "        #set id of new record into local variable\n",
    "        sensor_data_id = cursor.fetchone()[0]\n",
    "\n",
    "        if update == False:\n",
    "            #inserting a new sensor summary\n",
    "            cursor.execute(\"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES(%s, %s, %s, %s, %s)\", (int(mdf[0]), int(sensor_id),str(mdf[2]),int(sensor_data_id),int(mdf[3])))\n",
    "        else:\n",
    "            #updating sensor summary\n",
    "            cursor.execute(\"UPDATE sensor_network.sensor_summaries SET sensor_data_id = {} WHERE sensor_summaries.timestamp_start = {} AND sensor_summaries.sensor_id = {}\".format(int(sensor_data_id),int(mdf[0]), int(sensor_id)))\n",
    "        con.commit() \n",
    "\n",
    "        #inserting sensor summary\n",
    "        current_date = datetime.fromtimestamp(int(key.split('_')[0]))\n",
    "        cursor.execute(\"UPDATE sensor_network.sensors SET last_update = %s WHERE id = %s\", (current_date,int(sensor_id)))\n",
    "        con.commit() \n",
    "\n",
    "        cursor.close()\n",
    "    #if table name does not exist exit loop \n",
    "    except(psycopg2.errors.UndefinedTable) as error:\n",
    "        print('ERROR: ' + error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE 781\n",
      "INSERT\n",
      "INSERT\n",
      "UPDATE 784\n",
      "INSERT\n",
      "INSERT\n"
     ]
    }
   ],
   "source": [
    "previousSensorid = \"x\"\n",
    "upsert = True\n",
    "\n",
    "for key in sensor_summaries:\n",
    "\n",
    "    #split key to get only sensorid\n",
    "    s = key.split('_')[1]\n",
    "\n",
    "    # print(s)\n",
    "    # print(previousSensorid)\n",
    "    if s != previousSensorid:\n",
    "        previousSensorid = s\n",
    "        upsert = True\n",
    "    else:\n",
    "        upsert = False\n",
    "\n",
    "    #get the new key from sensors table\n",
    "    sensor_id = sensorsdf.loc[int(s)]['id']\n",
    "\n",
    "    mdf = sensor_summaries[key]\n",
    "    ldf = sensor_data[key]\n",
    "\n",
    "    if upsert == True:\n",
    "        try:\n",
    "            #Opening a cursor to execute database operations\n",
    "            cursor = con.cursor()\n",
    "            q = \"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES({}, {}, {}, NULL, {})\".format(int(mdf[0]), int(sensor_id),str(\"\\'\" + mdf[2] + \"\\'\"),int(mdf[3]))\n",
    "            q2 = \"\\n ON CONFLICT ON CONSTRAINT sensor_summaries_pkey\"\n",
    "            q3= \" \\n DO UPDATE SET b_box = {} WHERE sensor_summaries.timestamp_start = {} AND sensor_summaries.sensor_id = {} \\n RETURNING sensor_data_id;\".format(str(\"\\'\" + mdf[2] + \"\\'\"),int(mdf[0]), int(sensor_id))\n",
    "            query = q + q2 + q3\n",
    "            \n",
    "            cursor.execute(query)\n",
    "            con.commit() \n",
    "            result = cursor.fetchone()\n",
    "            #if the result fecthed is not empty then the sensor summary exsists and we can jsut update the raw measurement data\n",
    "            if result[0] != None:           \n",
    "                sensor_data_id = result[0]\n",
    "                #update sensor data\n",
    "                cursor.execute(\"UPDATE sensor_data.archive_measurements SET measurements = %s, locations = %s WHERE archive_measurements.id = %s;\", (str(ldf[0]),str(ldf[1]),int(sensor_data_id)))\n",
    "                con.commit()\n",
    "                print('UPDATE at', result[0])\n",
    "\n",
    "            else:\n",
    "                Insertdata(mdf,ldf,sensor_id,True)\n",
    "                print('INSERT')\n",
    "        \n",
    "        #if any errors occur then exit loop \n",
    "        except(Exception) as error:\n",
    "            print('ERROR: ' + error)\n",
    "            break\n",
    "\n",
    "    #regular insert\n",
    "    else: \n",
    "        Insertdata(mdf,ldf,sensor_id,False)\n",
    "        print('INSERT')\n",
    "\n",
    "#closing the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sensor_summaries:  \n",
    "\n",
    "\n",
    "#     #split key to get only sensorid\n",
    "#     s = key.split('_')[1]\n",
    "\n",
    "#     #get the new key from sensors table\n",
    "#     sensor_id = sensorsdf.loc[int(s)]['id']\n",
    "\n",
    "#     mdf = sensor_summaries[key]\n",
    "#     ldf = sensor_data[key]\n",
    "#     try:\n",
    "#             #Opening a cursor to execute database operations\n",
    "#             cursor = con.cursor()\n",
    "\n",
    "#             #inserting sensor data and return the id of new record\n",
    "#             cursor.execute(\"INSERT INTO sensor_data.archive_measurements (measurements,locations) VALUES(%s, %s) \\n RETURNING id\", (str(ldf[0]),str(ldf[1])) )\n",
    "#             con.commit() \n",
    "            \n",
    "#             #set id of new record into local variable\n",
    "#             sensor_data_id = cursor.fetchone()[0]\n",
    "\n",
    "#             #inserting sensor summary\n",
    "#             cursor.execute(\"INSERT INTO sensor_network.sensor_summaries (timestamp_start, sensor_id, b_box, sensor_data_id, measurement_count) VALUES(%s, %s, %s, %s, %s)\", (int(mdf[0]), int(sensor_id),str(mdf[2]),int(sensor_data_id),int(mdf[3])))\n",
    "#             con.commit() \n",
    "\n",
    "#             #inserting sensor summary\n",
    "#             current_date = datetime.fromtimestamp(int(key.split('_')[0]))\n",
    "#             cursor.execute(\"UPDATE sensor_network.sensors SET last_update = %s WHERE id = %s\", (current_date,int(sensor_id)))\n",
    "#             con.commit() \n",
    "\n",
    "#             cursor.close()\n",
    "#         #if table name does not exist exit loop \n",
    "#     except(psycopg2.errors.UndefinedTable) as error:\n",
    "#         print('ERROR: ' + error)\n",
    "#         break\n",
    "\n",
    "# #closing the connection\n",
    "# con.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58e6f625ba2c43b7fc3c95c61d4ef7749e639023949ab2a4f0956b7241794ddb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
